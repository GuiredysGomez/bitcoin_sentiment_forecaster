{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e70cc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import optuna   \n",
    "import json\n",
    "import os\n",
    "from optuna.visualization import plot_optimization_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661a9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    " # Se puede cambiar a \"precision\" o \"recall\" o \"f1-score\"\n",
    "SCORE = \"f1-score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb2404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d70ad",
   "metadata": {},
   "source": [
    "Score Trend Changes Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881b233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_changes_score(y_test: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the trend changes score based on the test and predicted values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        float: The trend changes score.\n",
    "    \"\"\"\n",
    "    y_df = pd.DataFrame([y_test, y_pred]).T\n",
    "    y_df.columns = [\"y_test\", \"y_pred\"]\n",
    "    y_df[\"y_test_shifted\"] = y_df[\"y_test\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_test\"] = y_df[\"y_test\"] != y_df[\"y_test_shifted\"]\n",
    "    y_df[\"y_predict_shifted\"] = y_df[\"y_pred\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_predict\"] = y_df[\"y_pred\"] != y_df[\"y_predict_shifted\"]\n",
    "    return classification_report(y_df[\"is_changed_trend_test\"][:-1], y_df[\"is_changed_trend_predict\"][:-1], digits=4)\n",
    "\n",
    "def trend_changes_true(y_test: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the trend changes score based on the test and predicted values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        float: The trend changes score.\n",
    "    \"\"\"\n",
    "    y_df = pd.DataFrame([y_test, y_pred]).T\n",
    "    y_df.columns = [\"y_test\", \"y_pred\"]\n",
    "    y_df[\"y_test_shifted\"] = y_df[\"y_test\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_test\"] = y_df[\"y_test\"] != y_df[\"y_test_shifted\"]\n",
    "    y_df[\"y_predict_shifted\"] = y_df[\"y_pred\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_predict\"] = y_df[\"y_pred\"] != y_df[\"y_predict_shifted\"]\n",
    "    report = classification_report(\n",
    "        y_df[\"is_changed_trend_test\"][:-1],\n",
    "        y_df[\"is_changed_trend_predict\"][:-1],\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return report[\"True\"][SCORE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8864b37",
   "metadata": {},
   "source": [
    "Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10aab5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "train = pd.read_csv(\"../../../data/training_set.csv\", parse_dates=[\"date\"])\n",
    "val = pd.read_csv(\"../../../data/validation_set.csv\", parse_dates=[\"date\"])\n",
    "test_set = pd.read_csv(\"../../../data/test_set.csv\", parse_dates=['date'])\n",
    "X_train = train.drop(columns=[\"date\", \"target_trend\"]).values\n",
    "y_train = train[\"target_trend\"].values\n",
    "X_val = val.drop(columns=[\"date\", \"target_trend\"]).values\n",
    "y_val = val[\"target_trend\"].values\n",
    "X_test = test_set.drop(columns=['target_trend','date']).values\n",
    "y_test = test_set['target_trend'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f49c3",
   "metadata": {},
   "source": [
    "IMPORTANTE: Hay que sumar a la columna de prediccion porque -1 no funciona en funcion de LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d625f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train += 1\n",
    "y_val += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfade2a3",
   "metadata": {},
   "source": [
    "Integracion de metrica trend_changes_score en la funcion objetivo Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edca5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seeds(SEED)  # Fijar semilla antes de cada trial\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", \"elasticnet\"])\n",
    "    C = trial.suggest_float(\"C\", 1e-3, 50, log=True)\n",
    "    l1_ratio = None\n",
    "    if penalty == \"elasticnet\":\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.1, 0.9)\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            multi_class=\"multinomial\",\n",
    "            solver=\"saga\",\n",
    "            penalty=penalty,\n",
    "            C=C,\n",
    "            l1_ratio=l1_ratio,\n",
    "            max_iter=800,\n",
    "            random_state=SEED\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    #score = trend_changes_true(y_val, y_pred)  # <- Métrica anterior\n",
    "    score = balanced_accuracy_score(y_val, y_pred) # <- Métrica actual\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c11435",
   "metadata": {},
   "source": [
    "Ejecucion de estudio con Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c2de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 09:52:30,909] A new study created in memory with name: no-name-10317641-c7dc-43d8-b924-3ffad6fdce88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:31,425] Trial 0 finished with value: 0.7714646464646465 and parameters: {'penalty': 'l2', 'C': 4.9020352232191025}. Best is trial 0 with value: 0.7714646464646465.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:32,069] Trial 1 finished with value: 0.752946127946128 and parameters: {'penalty': 'l1', 'C': 5.8610214330700705}. Best is trial 0 with value: 0.7714646464646465.\n",
      "[I 2025-08-21 09:52:32,363] Trial 2 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22602738844381381}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:32,503] Trial 3 finished with value: 0.7567340067340068 and parameters: {'penalty': 'l2', 'C': 0.43355484601640604}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:33,273] Trial 4 finished with value: 0.7714646464646465 and parameters: {'penalty': 'elasticnet', 'C': 14.044441691902366, 'l1_ratio': 0.3919087871210979}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:34,004] Trial 5 finished with value: 0.7714646464646465 and parameters: {'penalty': 'l1', 'C': 24.254780519039887}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:34,050] Trial 6 finished with value: 0.6986531986531986 and parameters: {'penalty': 'elasticnet', 'C': 0.030817927870550953, 'l1_ratio': 0.5544789221008554}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:34,071] Trial 7 finished with value: 0.3333333333333333 and parameters: {'penalty': 'l1', 'C': 0.004737573884223105}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:34,564] Trial 8 finished with value: 0.7714646464646465 and parameters: {'penalty': 'l2', 'C': 22.178146342804567}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:34,580] Trial 9 finished with value: 0.6346801346801346 and parameters: {'penalty': 'l2', 'C': 0.007344453390163737}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:34,852] Trial 10 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.298342049044871}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:35,247] Trial 11 finished with value: 0.752946127946128 and parameters: {'penalty': 'l2', 'C': 1.8339561401974636}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:35,960] Trial 12 finished with value: 0.7567340067340068 and parameters: {'penalty': 'l1', 'C': 1.0435811958194108}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:35,998] Trial 13 finished with value: 0.7087542087542088 and parameters: {'penalty': 'l2', 'C': 0.053710538303415045}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,122] Trial 14 finished with value: 0.7874579124579125 and parameters: {'penalty': 'elasticnet', 'C': 0.06966460109453439, 'l1_ratio': 0.8718273652123094}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,144] Trial 15 finished with value: 0.3333333333333333 and parameters: {'penalty': 'elasticnet', 'C': 0.0011781570836152347, 'l1_ratio': 0.871413203270236}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,276] Trial 16 finished with value: 0.7874579124579125 and parameters: {'penalty': 'elasticnet', 'C': 0.06151151807126983, 'l1_ratio': 0.8849741687622692}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,358] Trial 17 finished with value: 0.7049663299663299 and parameters: {'penalty': 'elasticnet', 'C': 0.12851733187067113, 'l1_ratio': 0.13164905322141435}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,406] Trial 18 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.013232060488248057}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,548] Trial 19 finished with value: 0.7828282828282828 and parameters: {'penalty': 'elasticnet', 'C': 0.1448462925219579, 'l1_ratio': 0.6316414175154188}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,854] Trial 20 finished with value: 0.7567340067340068 and parameters: {'penalty': 'elasticnet', 'C': 0.8654069634063034, 'l1_ratio': 0.7199080035444387}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:36,963] Trial 21 finished with value: 0.7874579124579125 and parameters: {'penalty': 'elasticnet', 'C': 0.05731592703243248, 'l1_ratio': 0.8903448846977242}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,035] Trial 22 finished with value: 0.7504208754208754 and parameters: {'penalty': 'elasticnet', 'C': 0.03827496112504783, 'l1_ratio': 0.76604025404323}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,139] Trial 23 finished with value: 0.7680976430976432 and parameters: {'penalty': 'elasticnet', 'C': 0.13156911172294686, 'l1_ratio': 0.341542607870272}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,190] Trial 24 finished with value: 0.7689393939393939 and parameters: {'penalty': 'elasticnet', 'C': 0.016079935719202635, 'l1_ratio': 0.7732742308807937}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,607] Trial 25 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.3494141102837442}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,806] Trial 26 finished with value: 0.7874579124579125 and parameters: {'penalty': 'elasticnet', 'C': 0.08773609884605783, 'l1_ratio': 0.877624016051602}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,819] Trial 27 finished with value: 0.3333333333333333 and parameters: {'penalty': 'l1', 'C': 0.00234370074583511}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:37,863] Trial 28 finished with value: 0.6986531986531986 and parameters: {'penalty': 'elasticnet', 'C': 0.01903377149335645, 'l1_ratio': 0.6499104807296628}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:38,492] Trial 29 finished with value: 0.752946127946128 and parameters: {'penalty': 'l1', 'C': 2.749014302302099}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:38,795] Trial 30 finished with value: 0.752946127946128 and parameters: {'penalty': 'elasticnet', 'C': 0.7353911233530726, 'l1_ratio': 0.35706318674195103}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:38,855] Trial 31 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.01182991215875475}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:39,082] Trial 32 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.027063769540405225}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:39,108] Trial 33 finished with value: 0.5513468013468014 and parameters: {'penalty': 'l1', 'C': 0.0071290653469708925}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:39,258] Trial 34 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.08283664387498205}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:39,603] Trial 35 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22406320615085942}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:40,015] Trial 36 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.25532658214744425}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:40,165] Trial 37 finished with value: 0.7828282828282828 and parameters: {'penalty': 'elasticnet', 'C': 0.19579388998167263, 'l1_ratio': 0.5245026893296921}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:40,664] Trial 38 finished with value: 0.7714646464646465 and parameters: {'penalty': 'l2', 'C': 5.174837928350518}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:40,447] Trial 39 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.4622391163700675}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:41,075] Trial 40 finished with value: 0.7714646464646465 and parameters: {'penalty': 'elasticnet', 'C': 9.735756634972448, 'l1_ratio': 0.7974964912930644}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:41,279] Trial 41 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.07177298460525891}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:41,340] Trial 42 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.033510070946294705}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:41,352] Trial 43 finished with value: 0.3333333333333333 and parameters: {'penalty': 'l1', 'C': 0.0033034496434434886}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:42,023] Trial 44 finished with value: 0.779040404040404 and parameters: {'penalty': 'l1', 'C': 0.5817934294520911}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:42,051] Trial 45 finished with value: 0.6531986531986531 and parameters: {'penalty': 'l2', 'C': 0.010544171978982204}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:42,312] Trial 46 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21076032146581702}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:42,585] Trial 47 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20411289036049915}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:43,264] Trial 48 finished with value: 0.7567340067340068 and parameters: {'penalty': 'l1', 'C': 2.1160651066029574}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:43,836] Trial 49 finished with value: 0.7668350168350168 and parameters: {'penalty': 'l1', 'C': 1.3023999692958046}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:44,100] Trial 50 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21983479374460427}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:44,362] Trial 51 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2003678314990994}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:44,605] Trial 52 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1949206585478024}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:44,842] Trial 53 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.3389573732903297}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:45,436] Trial 54 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.4813473780090976}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:45,578] Trial 55 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.11663730228697633}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:45,842] Trial 56 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2168270224449314}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:46,036] Trial 57 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1571236237592998}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:46,608] Trial 58 finished with value: 0.7668350168350168 and parameters: {'penalty': 'l1', 'C': 1.3410448304064266}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:47,221] Trial 59 finished with value: 0.7752525252525252 and parameters: {'penalty': 'l1', 'C': 0.6759846982008663}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:47,748] Trial 60 finished with value: 0.7714646464646465 and parameters: {'penalty': 'l2', 'C': 46.532351109345726}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:48,280] Trial 61 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2753015432965643}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:48,417] Trial 62 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.10886572131536287}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:48,617] Trial 63 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18459515964498735}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:49,265] Trial 64 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.42703773989464894}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:49,508] Trial 65 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.045914884691904356}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:49,824] Trial 66 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2948128321703699}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:50,048] Trial 67 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.10170982747280526}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:50,273] Trial 68 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17872369475543012}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:50,780] Trial 69 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.3745064073161712}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:51,423] Trial 70 finished with value: 0.7752525252525252 and parameters: {'penalty': 'l1', 'C': 0.915089179257361}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:51,716] Trial 71 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2234099003402014}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:51,873] Trial 72 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1438034405418291}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:52,355] Trial 73 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2617604838992333}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:52,941] Trial 74 finished with value: 0.7752525252525252 and parameters: {'penalty': 'l1', 'C': 0.6097239189076079}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:53,013] Trial 75 finished with value: 0.7344276094276094 and parameters: {'penalty': 'l2', 'C': 0.21466474800698926}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:53,258] Trial 76 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.06040400722379268}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:53,478] Trial 77 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.09598896250744907}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:53,646] Trial 78 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14766034038570022}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:53,756] Trial 79 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.024209608858429453}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:54,415] Trial 80 finished with value: 0.779040404040404 and parameters: {'penalty': 'l1', 'C': 0.507090723463271}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:54,661] Trial 81 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20346774183485392}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:54,849] Trial 82 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.32188275111867737}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:55,005] Trial 83 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.0774711399657939}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:55,225] Trial 84 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.16527608774830976}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:55,377] Trial 85 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.12398725586241283}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:55,618] Trial 86 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.04299138255290946}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:55,732] Trial 87 finished with value: 0.752946127946128 and parameters: {'penalty': 'l2', 'C': 0.3836286276695591}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:55,959] Trial 88 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20004380304989208}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:56,359] Trial 89 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.24975347845222387}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:56,523] Trial 90 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.08684296906318568}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:56,755] Trial 91 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1773059359691828}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:56,955] Trial 92 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18046529202700146}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:57,093] Trial 93 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.11809784111146354}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:57,288] Trial 94 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.32330788829777274}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:57,928] Trial 95 finished with value: 0.7752525252525252 and parameters: {'penalty': 'l1', 'C': 0.7794230320825022}. Best is trial 2 with value: 0.8013468013468014.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:52:58,579] Trial 96 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.44232394162868666}. Best is trial 2 with value: 0.8013468013468014.\n",
      "[I 2025-08-21 09:52:58,891] Trial 97 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23241163608599685}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:52:59,430] Trial 98 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2730935855084751}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:00,143] Trial 99 finished with value: 0.7668350168350168 and parameters: {'penalty': 'l1', 'C': 1.1446709090608873}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:00,182] Trial 100 finished with value: 0.7087542087542088 and parameters: {'penalty': 'l2', 'C': 0.05386231052557602}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:00,321] Trial 101 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13685919848243366}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:00,616] Trial 102 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2237092038193254}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:00,823] Trial 103 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.16972669143260552}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:01,456] Trial 104 finished with value: 0.779040404040404 and parameters: {'penalty': 'l1', 'C': 0.5653134965328882}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:01,975] Trial 105 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.38213431143714216}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:02,176] Trial 106 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.0729328573175243}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:02,314] Trial 107 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.10218691532913378}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:02,738] Trial 108 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2533197698400946}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:02,880] Trial 109 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14011804946383366}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:03,157] Trial 110 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2992700609676645}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:03,433] Trial 111 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21509133878717118}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:03,756] Trial 112 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23051612135821176}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:03,955] Trial 113 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1688659926881493}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:04,271] Trial 114 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.3427474767070806}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:04,365] Trial 115 finished with value: 0.6864478114478114 and parameters: {'penalty': 'elasticnet', 'C': 0.11547050158057022, 'l1_ratio': 0.10356425250770429}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:04,629] Trial 116 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21545441861369224}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:05,247] Trial 117 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.4561589181439862}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:05,738] Trial 118 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.27474732828563575}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:05,911] Trial 119 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15448181079846726}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:06,054] Trial 120 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.08456719233962323}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:06,394] Trial 121 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23053886365813217}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:06,748] Trial 122 finished with value: 0.7828282828282828 and parameters: {'penalty': 'l1', 'C': 0.24175025719646026}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:06,952] Trial 123 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17819695424936763}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:07,240] Trial 124 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.323961922024587}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:07,867] Trial 125 finished with value: 0.779040404040404 and parameters: {'penalty': 'l1', 'C': 0.5546196101221162}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:07,995] Trial 126 finished with value: 0.7567340067340068 and parameters: {'penalty': 'l2', 'C': 0.43879347136548547}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:08,127] Trial 127 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13661166183235743}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:08,359] Trial 128 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.09779987827421209}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:08,566] Trial 129 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18189230715861596}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:09,158] Trial 130 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.3915300720974809}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:09,457] Trial 131 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.22909513349240013}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:09,774] Trial 132 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22590356559075842}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:09,903] Trial 133 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.12101395928746689}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:10,323] Trial 134 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2825319358589775}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:10,559] Trial 135 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20462729842935032}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:10,698] Trial 136 finished with value: 0.7567340067340068 and parameters: {'penalty': 'elasticnet', 'C': 0.32631776593088296, 'l1_ratio': 0.23452334172258144}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:11,303] Trial 137 finished with value: 0.7752525252525252 and parameters: {'penalty': 'l1', 'C': 0.658878851570999}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:11,472] Trial 138 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15158183959866617}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:11,790] Trial 139 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.23347170262434025}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:11,977] Trial 140 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18368812483718866}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:12,488] Trial 141 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.26413805207431723}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:11,723] Trial 142 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13129371617084748}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:12,244] Trial 143 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.38322372229397855}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:12,487] Trial 144 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2021881165492149}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:12,624] Trial 145 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.10530888839891817}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:12,803] Trial 146 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15670637977856483}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:13,202] Trial 147 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2839972502071932}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:13,581] Trial 148 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.23890806317183141}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:13,635] Trial 149 finished with value: 0.7011784511784511 and parameters: {'penalty': 'l2', 'C': 0.06697190213749306}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:14,344] Trial 150 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.49617433905908565}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:14,587] Trial 151 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.197792298526196}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:14,798] Trial 152 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.3270003059495962}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:15,032] Trial 153 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1651854602430847}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:15,396] Trial 154 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23068276644258284}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:15,752] Trial 155 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.24583297682738794}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:15,883] Trial 156 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.12107785976079549}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:16,283] Trial 157 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.35681582660102595}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:16,543] Trial 158 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20411943013374378}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:16,709] Trial 159 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14441733380526}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:16,900] Trial 160 finished with value: 0.779040404040404 and parameters: {'penalty': 'elasticnet', 'C': 0.282261078964886, 'l1_ratio': 0.4601429692951101}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:17,113] Trial 161 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.19584651267874495}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:17,414] Trial 162 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22656920298188662}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:17,611] Trial 163 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1669729168180667}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:17,865] Trial 164 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.30318558277088403}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:18,500] Trial 165 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.40654668646513203}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:18,630] Trial 166 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.10548480038897025}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:18,643] Trial 167 finished with value: 0.3333333333333333 and parameters: {'penalty': 'l1', 'C': 0.0013053964145894707}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:19,044] Trial 168 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.24421363512127658}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:19,180] Trial 169 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13658313001077563}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:19,406] Trial 170 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.19274686630855764}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:19,614] Trial 171 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1899456649849556}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:20,111] Trial 172 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2721021357068136}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:20,942] Trial 173 finished with value: 0.752946127946128 and parameters: {'penalty': 'l1', 'C': 3.777942719297184}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:21,134] Trial 174 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.16710987177895573}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:21,415] Trial 175 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22182575280331301}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:21,643] Trial 176 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.3299661300488581}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:21,846] Trial 177 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14252049332194466}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:21,902] Trial 178 finished with value: 0.7011784511784511 and parameters: {'penalty': 'l2', 'C': 0.0886628966832758}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:22,186] Trial 179 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22048670347983773}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:22,325] Trial 180 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1228930429495407}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:22,552] Trial 181 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17622625250026053}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:22,883] Trial 182 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.29718451378134325}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:23,148] Trial 183 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17736629623999323}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:23,570] Trial 184 finished with value: 0.7828282828282828 and parameters: {'penalty': 'l1', 'C': 0.2422702164184281}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:23,743] Trial 185 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15351013304419667}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:24,122] Trial 186 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.35885304656931244}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:24,353] Trial 187 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.19898764498841148}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:24,841] Trial 188 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.26484393237398846}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:25,476] Trial 189 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.4553598631281922}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:26,170] Trial 190 finished with value: 0.7714646464646465 and parameters: {'penalty': 'elasticnet', 'C': 9.424436189235614, 'l1_ratio': 0.22568800511020443}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:26,357] Trial 191 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18154750128220798}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:26,491] Trial 192 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13214699032322966}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:26,748] Trial 193 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21228440020026473}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:26,933] Trial 194 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1590428401184069}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:27,430] Trial 195 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2631654153186363}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:27,575] Trial 196 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.11722135293164508}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:27,858] Trial 197 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22131071799312202}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:28,035] Trial 198 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.31746752994129784}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:28,239] Trial 199 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1699276123049695}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:28,473] Trial 200 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20520498579448132}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:28,773] Trial 201 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23108258485884808}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:29,218] Trial 202 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2583976881058116}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:29,408] Trial 203 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15220559581623994}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:29,883] Trial 204 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.375255306213812}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:30,147] Trial 205 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1954492326835812}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:30,586] Trial 206 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2941695716274111}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:30,936] Trial 207 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.23330394651204142}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:31,075] Trial 208 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13235037310192715}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:31,335] Trial 209 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17706604140198254}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:31,394] Trial 210 finished with value: 0.7011784511784511 and parameters: {'penalty': 'l2', 'C': 0.10313198102364295}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:31,731] Trial 211 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22411002336823166}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:32,256] Trial 212 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2619612360899066}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:32,482] Trial 213 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.19777187815933983}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:32,778] Trial 214 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.3427123601797448}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:32,973] Trial 215 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15985615454063037}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:33,314] Trial 216 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23083985024933204}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:33,705] Trial 217 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2892662426893065}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:34,089] Trial 218 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.23679270398281732}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:34,294] Trial 219 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18548053442844575}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:34,469] Trial 220 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14543357721801226}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:34,821] Trial 221 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.23572514169254333}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:35,079] Trial 222 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.303578263049904}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:35,370] Trial 223 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2107563235940435}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:35,645] Trial 224 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.16972178863688273}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:36,063] Trial 225 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2533182250007189}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:36,286] Trial 226 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.1894244383985083}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:36,424] Trial 227 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1340437889228936}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:36,573] Trial 228 finished with value: 0.7567340067340068 and parameters: {'penalty': 'elasticnet', 'C': 0.34753845708009073, 'l1_ratio': 0.25091189769606825}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:36,859] Trial 229 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22223836560635682}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:37,381] Trial 230 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2761629763637525}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:37,643] Trial 231 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17595278667424982}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:37,847] Trial 232 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15393153004737328}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:38,156] Trial 233 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21896251371080508}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:38,366] Trial 234 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18769347930326538}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:38,507] Trial 235 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.11976125220348775}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:39,036] Trial 236 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2669899277972687}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:39,293] Trial 237 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21161632181658252}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:39,482] Trial 238 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15888195372475597}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:39,718] Trial 239 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.30352440995920876}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:40,362] Trial 240 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.4052688528250777}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:40,627] Trial 241 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2134106658592683}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:41,066] Trial 242 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.23798822336540193}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:41,269] Trial 243 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18394218970600568}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:41,473] Trial 244 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13553421026081564}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:42,019] Trial 245 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.25863524476656696}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:42,260] Trial 246 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.16621054542886582}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:42,523] Trial 247 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.21090499645298705}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,074] Trial 248 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.27426128607412104}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,152] Trial 249 finished with value: 0.7344276094276094 and parameters: {'penalty': 'l2', 'C': 0.18380459727594273}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,373] Trial 250 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.3281030041650094}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,557] Trial 251 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14610567290322515}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,005] Trial 252 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.2247203504885181}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,145] Trial 253 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.11373742514301903}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,366] Trial 254 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18738009441324155}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:43,772] Trial 255 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.24398199936971476}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:44,020] Trial 256 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1528093620491353}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:44,214] Trial 257 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.31433218554439063}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:44,465] Trial 258 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20496364472404438}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:44,652] Trial 259 finished with value: 0.779040404040404 and parameters: {'penalty': 'elasticnet', 'C': 0.2795860526394753, 'l1_ratio': 0.433728647684842}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:44,879] Trial 260 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.16740233265911486}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:45,368] Trial 261 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.37051935461387464}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:45,849] Trial 262 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2507977581162253}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:46,000] Trial 263 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.13567047157440243}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:46,274] Trial 264 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20603930020422706}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:46,525] Trial 265 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.16874841909966612}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:46,749] Trial 266 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.09463850601503412}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:47,135] Trial 267 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22719299103392226}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:47,358] Trial 268 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.11802841085061813}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:47,674] Trial 269 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.2971251416061332}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:47,887] Trial 270 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.19008410841681883}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:47,966] Trial 271 finished with value: 0.7344276094276094 and parameters: {'penalty': 'l2', 'C': 0.24339279057804458}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:48,602] Trial 272 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.43881447601202644}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:49,256] Trial 273 finished with value: 0.7714646464646465 and parameters: {'penalty': 'l1', 'C': 26.00096521795589}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:49,435] Trial 274 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15036080120887535}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:49,822] Trial 275 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.35116146264489534}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:50,044] Trial 276 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.19086196484430842}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:50,513] Trial 277 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2762484548959236}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:50,828] Trial 278 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.22390807442147656}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:50,976] Trial 279 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.1310843870537279}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:51,175] Trial 280 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.16130121296247146}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:51,362] Trial 281 finished with value: 0.779040404040404 and parameters: {'penalty': 'elasticnet', 'C': 0.2696348922568499, 'l1_ratio': 0.6127285927489825}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:51,593] Trial 282 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20195560759702338}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:51,808] Trial 283 finished with value: 0.7643097643097643 and parameters: {'penalty': 'l1', 'C': 0.30836481996922377}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:52,044] Trial 284 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17452905831445334}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:52,371] Trial 285 finished with value: 0.8051346801346803 and parameters: {'penalty': 'l1', 'C': 0.23170774729926547}. Best is trial 97 with value: 0.8051346801346803.\n",
      "/home/toyotx22/bitcoin_sentiment_forecaster/tf_venv2/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-08-21 09:53:53,111] Trial 286 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.4305083822308797}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:53,141] Trial 287 finished with value: 0.3333333333333333 and parameters: {'penalty': 'l1', 'C': 0.005031901836271039}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:53,551] Trial 288 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.24010580138289234}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:53,971] Trial 289 finished with value: 0.7605218855218855 and parameters: {'penalty': 'l1', 'C': 0.36029169722816756}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:54,118] Trial 290 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.14043316712045104}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:54,312] Trial 291 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.18196243086437172}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:54,460] Trial 292 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.10877543232327518}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:54,819] Trial 293 finished with value: 0.7866161616161617 and parameters: {'penalty': 'l1', 'C': 0.24028158637276598}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:54,925] Trial 294 finished with value: 0.752946127946128 and parameters: {'penalty': 'l2', 'C': 0.31527259158187565}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:55,122] Trial 295 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.15272282410941054}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:55,384] Trial 296 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.20309052320081608}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:55,856] Trial 297 finished with value: 0.7680976430976432 and parameters: {'penalty': 'l1', 'C': 0.2575802683830895}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:56,109] Trial 298 finished with value: 0.8013468013468014 and parameters: {'penalty': 'l1', 'C': 0.17646220046698105}. Best is trial 97 with value: 0.8051346801346803.\n",
      "[I 2025-08-21 09:53:56,254] Trial 299 finished with value: 0.7874579124579125 and parameters: {'penalty': 'l1', 'C': 0.12632818502450874}. Best is trial 97 with value: 0.8051346801346803.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=SEED)\n",
    ")\n",
    "study.optimize(objective, n_trials=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa087a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          0.7714646464646465,
          0.752946127946128,
          0.8013468013468014,
          0.7567340067340068,
          0.7714646464646465,
          0.7714646464646465,
          0.6986531986531986,
          0.3333333333333333,
          0.7714646464646465,
          0.6346801346801346,
          0.7643097643097643,
          0.752946127946128,
          0.7567340067340068,
          0.7087542087542088,
          0.7874579124579125,
          0.3333333333333333,
          0.7874579124579125,
          0.7049663299663299,
          0.7874579124579125,
          0.7828282828282828,
          0.7567340067340068,
          0.7874579124579125,
          0.7504208754208754,
          0.7680976430976432,
          0.7689393939393939,
          0.7605218855218855,
          0.7874579124579125,
          0.3333333333333333,
          0.6986531986531986,
          0.752946127946128,
          0.752946127946128,
          0.7874579124579125,
          0.7874579124579125,
          0.5513468013468014,
          0.7874579124579125,
          0.8013468013468014,
          0.7680976430976432,
          0.7828282828282828,
          0.7714646464646465,
          0.7605218855218855,
          0.7714646464646465,
          0.7874579124579125,
          0.7874579124579125,
          0.3333333333333333,
          0.779040404040404,
          0.6531986531986531,
          0.8013468013468014,
          0.8013468013468014,
          0.7567340067340068,
          0.7668350168350168,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.7605218855218855,
          0.7605218855218855,
          0.7874579124579125,
          0.8013468013468014,
          0.7874579124579125,
          0.7668350168350168,
          0.7752525252525252,
          0.7714646464646465,
          0.7680976430976432,
          0.7874579124579125,
          0.8013468013468014,
          0.7605218855218855,
          0.7874579124579125,
          0.7643097643097643,
          0.7874579124579125,
          0.8013468013468014,
          0.7605218855218855,
          0.7752525252525252,
          0.8013468013468014,
          0.7874579124579125,
          0.7680976430976432,
          0.7752525252525252,
          0.7344276094276094,
          0.7874579124579125,
          0.7874579124579125,
          0.7874579124579125,
          0.7874579124579125,
          0.779040404040404,
          0.8013468013468014,
          0.7643097643097643,
          0.7874579124579125,
          0.7874579124579125,
          0.7874579124579125,
          0.7874579124579125,
          0.752946127946128,
          0.8013468013468014,
          0.7680976430976432,
          0.7874579124579125,
          0.8013468013468014,
          0.8013468013468014,
          0.7874579124579125,
          0.7643097643097643,
          0.7752525252525252,
          0.7605218855218855,
          0.8051346801346803,
          0.7680976430976432,
          0.7668350168350168,
          0.7087542087542088,
          0.7874579124579125,
          0.8013468013468014,
          0.8013468013468014,
          0.779040404040404,
          0.7605218855218855,
          0.7874579124579125,
          0.7874579124579125,
          0.7680976430976432,
          0.7874579124579125,
          0.7643097643097643,
          0.8013468013468014,
          0.8051346801346803,
          0.8013468013468014,
          0.7605218855218855,
          0.6864478114478114,
          0.8013468013468014,
          0.7605218855218855,
          0.7680976430976432,
          0.7874579124579125,
          0.7874579124579125,
          0.8051346801346803,
          0.7828282828282828,
          0.8013468013468014,
          0.7643097643097643,
          0.779040404040404,
          0.7567340067340068,
          0.7874579124579125,
          0.7874579124579125,
          0.8013468013468014,
          0.7605218855218855,
          0.8051346801346803,
          0.8013468013468014,
          0.7874579124579125,
          0.7643097643097643,
          0.8013468013468014,
          0.7567340067340068,
          0.7752525252525252,
          0.7874579124579125,
          0.7866161616161617,
          0.8013468013468014,
          0.7680976430976432,
          0.7874579124579125,
          0.7605218855218855,
          0.8013468013468014,
          0.7874579124579125,
          0.7874579124579125,
          0.7643097643097643,
          0.7866161616161617,
          0.7011784511784511,
          0.7605218855218855,
          0.8013468013468014,
          0.7643097643097643,
          0.7874579124579125,
          0.8051346801346803,
          0.7866161616161617,
          0.7874579124579125,
          0.7605218855218855,
          0.8013468013468014,
          0.7874579124579125,
          0.779040404040404,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.7643097643097643,
          0.7605218855218855,
          0.7874579124579125,
          0.3333333333333333,
          0.7866161616161617,
          0.7874579124579125,
          0.8013468013468014,
          0.8013468013468014,
          0.7680976430976432,
          0.752946127946128,
          0.8013468013468014,
          0.8013468013468014,
          0.7643097643097643,
          0.7874579124579125,
          0.7011784511784511,
          0.8013468013468014,
          0.7874579124579125,
          0.8013468013468014,
          0.7643097643097643,
          0.8013468013468014,
          0.7828282828282828,
          0.7874579124579125,
          0.7605218855218855,
          0.8013468013468014,
          0.7680976430976432,
          0.7605218855218855,
          0.7714646464646465,
          0.8013468013468014,
          0.7874579124579125,
          0.8013468013468014,
          0.7874579124579125,
          0.7680976430976432,
          0.7874579124579125,
          0.8013468013468014,
          0.7643097643097643,
          0.8013468013468014,
          0.8013468013468014,
          0.8051346801346803,
          0.7680976430976432,
          0.7874579124579125,
          0.7605218855218855,
          0.8013468013468014,
          0.7643097643097643,
          0.7866161616161617,
          0.7874579124579125,
          0.8013468013468014,
          0.7011784511784511,
          0.8013468013468014,
          0.7680976430976432,
          0.8013468013468014,
          0.7605218855218855,
          0.7874579124579125,
          0.8051346801346803,
          0.7643097643097643,
          0.7866161616161617,
          0.8013468013468014,
          0.7874579124579125,
          0.7866161616161617,
          0.7643097643097643,
          0.8013468013468014,
          0.8013468013468014,
          0.7680976430976432,
          0.8013468013468014,
          0.7874579124579125,
          0.7567340067340068,
          0.8013468013468014,
          0.7680976430976432,
          0.8013468013468014,
          0.7874579124579125,
          0.8013468013468014,
          0.8013468013468014,
          0.7874579124579125,
          0.7680976430976432,
          0.8013468013468014,
          0.7874579124579125,
          0.7643097643097643,
          0.7605218855218855,
          0.8013468013468014,
          0.7866161616161617,
          0.8013468013468014,
          0.7874579124579125,
          0.7680976430976432,
          0.7874579124579125,
          0.8013468013468014,
          0.7680976430976432,
          0.7344276094276094,
          0.7643097643097643,
          0.7874579124579125,
          0.8013468013468014,
          0.7874579124579125,
          0.8013468013468014,
          0.7866161616161617,
          0.7874579124579125,
          0.7643097643097643,
          0.8013468013468014,
          0.779040404040404,
          0.8013468013468014,
          0.7605218855218855,
          0.7680976430976432,
          0.7874579124579125,
          0.8013468013468014,
          0.8013468013468014,
          0.7874579124579125,
          0.8013468013468014,
          0.7874579124579125,
          0.7643097643097643,
          0.8013468013468014,
          0.7344276094276094,
          0.7605218855218855,
          0.7714646464646465,
          0.7874579124579125,
          0.7605218855218855,
          0.8013468013468014,
          0.7680976430976432,
          0.8013468013468014,
          0.7874579124579125,
          0.7874579124579125,
          0.779040404040404,
          0.8013468013468014,
          0.7643097643097643,
          0.8013468013468014,
          0.8051346801346803,
          0.7605218855218855,
          0.3333333333333333,
          0.7866161616161617,
          0.7605218855218855,
          0.7874579124579125,
          0.8013468013468014,
          0.7874579124579125,
          0.7866161616161617,
          0.752946127946128,
          0.7874579124579125,
          0.8013468013468014,
          0.7680976430976432,
          0.8013468013468014,
          0.7874579124579125
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          0.7714646464646465,
          0.7714646464646465,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8013468013468014,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803,
          0.8051346801346803
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizar el historial de optimización\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41699610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados:\n",
      "{'penalty': 'l1', 'C': 0.23241163608599685}\n",
      "Mejor score de f1-score: 0.8051\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "print(f\"Mejor score de {SCORE}: {study.best_value:.4f}\")\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49238aa",
   "metadata": {},
   "source": [
    "GUARDAR EN JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Guardar los mejores hiperparámetros y su valor\n",
    "# history = []\n",
    "# if os.path.exists(\"best_hyperparams.json\"):\n",
    "#     try:\n",
    "#         with open(\"best_hyperparams.json\", \"r\") as f:\n",
    "#             history = json.load(f)\n",
    "#     except (json.JSONDecodeError, ValueError):\n",
    "#         history = []\n",
    "\n",
    "# # Guardar ambos en un solo diccionario\n",
    "# history.append({\n",
    "#     \"params\": study.best_params,\n",
    "#     \"value\": study.best_value\n",
    "# })\n",
    "\n",
    "# with open(\"best_hyperparams.json\", \"w\") as f:\n",
    "#     json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749d589",
   "metadata": {},
   "source": [
    "CARGAR HIPERPARAMETROS DESDE JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cfa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros #1: {'penalty': 'l1', 'C': 0.23241163608599685}, Valor: 0.8051346801346803\n"
     ]
    }
   ],
   "source": [
    "# # Cargar historial de hiperparámetros y valores\n",
    "# with open(\"best_hyperparams.json\", \"r\") as f:\n",
    "#     history = json.load(f)\n",
    "\n",
    "# # Escoger el último (más reciente)\n",
    "# best_params = history[-1][\"params\"]\n",
    "# best_value = history[-1][\"value\"]\n",
    "\n",
    "# # Si quieres ver todos:\n",
    "# for i, entry in enumerate(history):\n",
    "#     print(f\"Hiperparámetros #{i+1}: {entry['params']}, Valor: {entry['value']}\")\n",
    "\n",
    "# # Si quieres escoger uno específico (por índice):\n",
    "# # best_params = history[indice_que_quieras][\"params\"]\n",
    "# # best_value = history[indice_que_quieras][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7dc6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Change F1 Score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False     0.8761    0.8839    0.8800       112\n",
      "        True     0.1875    0.1765    0.1818        17\n",
      "\n",
      "    accuracy                         0.7907       129\n",
      "   macro avg     0.5318    0.5302    0.5309       129\n",
      "weighted avg     0.7854    0.7907    0.7880       129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar modelo final con los mejores hiperparámetros\n",
    "set_seeds(SEED)\n",
    "best_params = study.best_params\n",
    "final_l1_ratio = best_params[\"l1_ratio\"] if best_params[\"penalty\"] == \"elasticnet\" else None\n",
    "final_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"saga\",\n",
    "        penalty=best_params[\"penalty\"],\n",
    "        C=best_params[\"C\"],\n",
    "        l1_ratio=final_l1_ratio,\n",
    "        max_iter=800,\n",
    "        random_state=SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred = final_model.predict(X_val)\n",
    "print(\"Trend Change F1 Score:\\n\", trend_changes_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20220f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8800    0.9167    0.8980        24\n",
      "           1     0.5882    0.5556    0.5714        18\n",
      "           2     0.9432    0.9432    0.9432        88\n",
      "\n",
      "    accuracy                         0.8846       130\n",
      "   macro avg     0.8038    0.8051    0.8042       130\n",
      "weighted avg     0.8824    0.8846    0.8834       130\n",
      "\n",
      "Balanced accuracy: 0.8051346801346803\n"
     ]
    }
   ],
   "source": [
    "# Reporte completo: precisión, recall y F1 por clase\n",
    "report = classification_report(y_val, y_pred, digits=4)\n",
    "print(\"LightGBM Report:\\n\", report)\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "867b05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Change F1 Score:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False     0.9310    0.9076    0.9191       119\n",
      "        True     0.2143    0.2727    0.2400        11\n",
      "\n",
      "    accuracy                         0.8538       130\n",
      "   macro avg     0.5727    0.5901    0.5796       130\n",
      "weighted avg     0.8704    0.8538    0.8617       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtener predicciones\n",
    "y_pred = final_model.predict(X_test)\n",
    "print(\"Trend Change F1 Score:\\n\", trend_changes_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv2 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
