{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3de6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, f1_score\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2df96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Naive Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccdc18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend_changes_report_dict(y_test: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the trend changes score based on the test and predicted values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        float: The trend changes score.\n",
    "    \"\"\"\n",
    "    y_df = pd.DataFrame([y_test, y_pred]).T\n",
    "    y_df.columns = [\"y_test\", \"y_pred\"]\n",
    "    y_df[\"y_test_shifted\"] = y_df[\"y_test\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_test\"] = y_df[\"y_test\"] != y_df[\"y_test_shifted\"]\n",
    "    y_df[\"y_predict_shifted\"] = y_df[\"y_pred\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_predict\"] = y_df[\"y_pred\"] != y_df[\"y_predict_shifted\"]\n",
    "    return classification_report(\n",
    "        y_df[\"is_changed_trend_test\"][:-1], \n",
    "        y_df[\"is_changed_trend_predict\"][:-1], \n",
    "        digits=4,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "def trend_changes_score(y_test: np.array, y_pred: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the trend changes score based on the test and predicted values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (np.array): True labels.\n",
    "        y_pred (np.array): Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        float: The trend changes score.\n",
    "    \"\"\"\n",
    "    y_df = pd.DataFrame([y_test, y_pred]).T\n",
    "    y_df.columns = [\"y_test\", \"y_pred\"]\n",
    "    y_df[\"y_test_shifted\"] = y_df[\"y_test\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_test\"] = y_df[\"y_test\"] != y_df[\"y_test_shifted\"]\n",
    "    y_df[\"y_predict_shifted\"] = y_df[\"y_pred\"].shift(-1)\n",
    "    y_df[\"is_changed_trend_predict\"] = y_df[\"y_pred\"] != y_df[\"y_predict_shifted\"]\n",
    "    return classification_report(y_df[\"is_changed_trend_test\"][:-1], y_df[\"is_changed_trend_predict\"][:-1], digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b55f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"../../../data/post_cleaning/training_set.csv\")\n",
    "validation_set = pd.read_csv(\"../../../data/post_cleaning/validation_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2050dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = validation_set['target_trend']\n",
    "y_val_pred = validation_set['trend_d0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f6feb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveModel report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.8242    0.8152    0.8197        92\n",
      "           0     0.3469    0.3469    0.3469        49\n",
      "           1     0.8312    0.8366    0.8339       153\n",
      "\n",
      "    accuracy                         0.7483       294\n",
      "   macro avg     0.6674    0.6663    0.6668       294\n",
      "weighted avg     0.7483    0.7483    0.7483       294\n",
      "\n",
      "Balanced accuracy: 0.6662524913346982\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_val, y_val_pred, digits=4)\n",
    "print(f\"NaiveModel report:\\n {report}\")\n",
    "print(\"Balanced accuracy:\", balanced_accuracy_score(y_val, y_val_pred))\n",
    "\n",
    "opt_duration_sec = 0  # No hubo optimización en el modelo ingenuo\n",
    "\n",
    "n_trials_run = 0  # No hubo trials en el modelo ingenuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d024c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False     0.8219    0.8219    0.8219       219\n",
      "        True     0.4730    0.4730    0.4730        74\n",
      "\n",
      "    accuracy                         0.7338       293\n",
      "   macro avg     0.6474    0.6474    0.6474       293\n",
      "weighted avg     0.7338    0.7338    0.7338       293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trend_changes_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a60bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exportar y Comparar Métricas de Modelos (Validación) ---\n",
    "\n",
    "# 1. Definir el nombre del modelo actual y el archivo de salida\n",
    "output_file = '../../../score_models/model_comparison_metrics.csv'\n",
    "\n",
    "# 2. Calcular el reporte de clasificación estándar\n",
    "report_dict = classification_report(y_val, y_val_pred, output_dict=True, zero_division=0)\n",
    "precision = report_dict['macro avg']['precision']\n",
    "recall = report_dict['macro avg']['recall']\n",
    "f1_score = report_dict['macro avg']['f1-score']\n",
    "\n",
    "\n",
    "# 3. Calcular el reporte de cambio de tendencia\n",
    "report = get_trend_changes_report_dict(y_val, y_val_pred)\n",
    "trend_change_precision = report['True']['precision']\n",
    "trend_change_recall = report['True']['recall']\n",
    "trend_change_f1_score = report['True']['f1-score']\n",
    "\n",
    "# 4. Organizar las nuevas métricas\n",
    "new_metrics = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1_score,\n",
    "    'trend_change_precision': trend_change_precision,\n",
    "    'trend_change_recall': trend_change_recall,\n",
    "    'trend_change_f1_score': trend_change_f1_score,\n",
    "    \"optuna_duration_sec\": opt_duration_sec,\n",
    "    \"n_trials\": n_trials_run,\n",
    "}\n",
    "\n",
    "# 5. Cargar, actualizar y guardar el DataFrame de comparación\n",
    "try:\n",
    "    # Intentar cargar el archivo existente\n",
    "    comparison_df = pd.read_csv(output_file, index_col='model')\n",
    "    # Si existe, actualizar o añadir la fila para el modelo actual\n",
    "    comparison_df.loc[model_name] = new_metrics\n",
    "except FileNotFoundError:\n",
    "    # Si no existe, crear un DataFrame nuevo directamente con los datos actuales\n",
    "    comparison_df = pd.DataFrame([new_metrics], index=[model_name])\n",
    "\n",
    "# Guardar el DataFrame actualizado en el CSV\n",
    "comparison_df.to_csv(output_file, index_label='model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_venv2 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
