import torch
import time
import pandas as pd
from transformers import TextClassificationPipeline, AutoModelForSequenceClassification, AutoTokenizer


start_time = time.time()  # ğŸ”¹ Inicia el temporizador
# ğŸ”¹ Verificar si CUDA estÃ¡ disponible
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Usando: {device}")

# ğŸ”¹ Cargar modelo CryptoBERT desde Hugging Face y asignarlo a la GPU
model_name = "ElKulako/cryptobert"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
 # ğŸ”¹ Mueve el modelo a GPU
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)

# ğŸ”¹ Crear pipeline de clasificaciÃ³n optimizado con CUDA
pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0 if device == "cuda" else -1, max_length=64, truncation=True, padding="max_length")

class SentimentAnalyzer:
    def batch_analyze(self, texts):
        """Analiza sentimiento en GPU con procesamiento en lotes (batch processing)"""
        responses = pipe(texts, batch_size=32)  # ğŸ”¹ Procesa 32 tweets en cada iteraciÃ³n
        return [self._parse_response(res) for res in responses]

    def _parse_response(self, response):
        """Convierte etiquetas de CryptoBERT a valores numÃ©ricos"""
        if "Bullish" in response["label"]: return 1
        elif "Bearish" in response["label"]: return -1
        else: return 0

# ğŸ”¹ Cargar CSV con tweets
csv_path = "../data_extractor/tweets_news.csv"
df = pd.read_csv(csv_path)

# ğŸ”¹ Procesar tweets con batch processing en GPU
analyzer = SentimentAnalyzer()
df["Sentiment"] = analyzer.batch_analyze(df["Text"].tolist())  # ğŸ”¹ Se procesa por lotes

# ğŸ”¹ Guardar resultados en un nuevo archivo
df.to_csv("../data/tweets_news_with_sentiment_cryptobert.csv", index=False)

print("âœ… Procesamiento completado con CryptoBERT en GPU. Datos guardados.")
end_time = time.time()
print(f"â±ï¸ Execution time: {end_time - start_time:.4f} seconds")